{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Input, Lambda, Dense,Dropout,SpatialDropout1D,Conv1D,GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"fake_real_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>news_url</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>699</td>\n",
       "      <td>bustle.com</td>\n",
       "      <td>are rachel bryan still together in the bachelo...</td>\n",
       "      <td>1</td>\n",
       "      <td>bustle.com are rachel bryan still together in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15378</td>\n",
       "      <td>people.com</td>\n",
       "      <td>all about the crib kim kardashian west and bey...</td>\n",
       "      <td>1</td>\n",
       "      <td>people.com all about the crib kim kardashian w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10359</td>\n",
       "      <td>bbc.co.uk</td>\n",
       "      <td>royal wedding meghan markle s givenchy dress i...</td>\n",
       "      <td>1</td>\n",
       "      <td>bbc.co.uk royal wedding meghan markle s givenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10551</td>\n",
       "      <td>people.com</td>\n",
       "      <td>anna wintour calls scarlett johansson wearing ...</td>\n",
       "      <td>1</td>\n",
       "      <td>people.com anna wintour calls scarlett johanss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15248</td>\n",
       "      <td>usmagazine.com</td>\n",
       "      <td>stassi schroeder apologizes for ntroversial me...</td>\n",
       "      <td>1</td>\n",
       "      <td>usmagazine.com stassi schroeder apologizes for...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        news_url  \\\n",
       "0         699      bustle.com   \n",
       "1       15378      people.com   \n",
       "2       10359       bbc.co.uk   \n",
       "3       10551      people.com   \n",
       "4       15248  usmagazine.com   \n",
       "\n",
       "                                               title  label  \\\n",
       "0  are rachel bryan still together in the bachelo...      1   \n",
       "1  all about the crib kim kardashian west and bey...      1   \n",
       "2  royal wedding meghan markle s givenchy dress i...      1   \n",
       "3  anna wintour calls scarlett johansson wearing ...      1   \n",
       "4  stassi schroeder apologizes for ntroversial me...      1   \n",
       "\n",
       "                                              merged  \n",
       "0  bustle.com are rachel bryan still together in ...  \n",
       "1  people.com all about the crib kim kardashian w...  \n",
       "2  bbc.co.uk royal wedding meghan markle s givenc...  \n",
       "3  people.com anna wintour calls scarlett johanss...  \n",
       "4  usmagazine.com stassi schroeder apologizes for...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[\"title\"].values\n",
    "Y = dataset[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new data\n",
    "fake = pd.read_csv(\"gossipcop_fake.csv\")\n",
    "real = pd.read_csv(\"gossipcop_real.csv\")\n",
    "fake_t = fake.title\n",
    "real_t = real.title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "#embed = hub.text_embedding_column(key='sentence',module_spec=url,)\n",
    "embedding_feature = hub.text_embedding_column(\n",
    "    key='sentence', \n",
    "    module_spec=\"https://tfhub.dev/google/universal-sentence-encoder-large/3\",\n",
    "    trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating custom layer\n",
    "def UniversalEmbedding(x):\n",
    "    return embedding_feature(tf.squeeze(tf.cast(x,tf.string)))\n",
    "#creating a Attention layer\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the model using the functional keras api\n",
    "input_text = Input(shape=(1,),dtype=tf.string)\n",
    "embedding_feature = Lambda(UniversalEmbedding,output_shape=(512,))(input_text)\n",
    "drop_1 = Dropout(0.2)(embedding)\n",
    "#cnn_1 = Conv1D(256,3,padding='valid',activation='relu',strides=1)(drop_1)\n",
    "#max_pool = GlobalMaxPooling1D()(cnn_1)\n",
    "#att_1 = Attention()(max_pool)\n",
    "dense = Dense(128, activation='relu')(drop_1)\n",
    "drop_2 = Dropout(0.5)(dense)\n",
    "pred = Dense(1, activation='softmax')(drop_2)\n",
    "model = Model(inputs=[input_text], outputs=pred)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17712 samples, validate on 4428 samples\n",
      "Epoch 1/1\n",
      "17712/17712 [==============================] - 219s 12ms/step - loss: 3.8245 - acc: 0.7601 - val_loss: 3.8668 - val_acc: 0.7575\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())  \n",
    "    session.run(tf.tables_initializer())\n",
    "    history = model.fit(X, Y, epochs=1, batch_size=32,validation_split=0.2)\n",
    "    model.save_weights('./model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using tf estimator\n",
    "# Training input on the whole training set with no limit on training epochs.\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': X_train}, y_train, \n",
    "    batch_size=256, num_epochs=None, shuffle=True)\n",
    "    \n",
    "# Prediction on the whole training set.\n",
    "predict_train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': X_train}, y_train, shuffle=False)\n",
    "    \n",
    "# Prediction on the whole validation set.\n",
    "predict_val_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': X_test}, y_test, shuffle=False)\n",
    "    \n",
    "# Prediction on the test set.\n",
    "predict_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {'sentence': X_test}, y_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\anubh\\AppData\\Local\\Temp\\tmpv7o4_dcu\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\anubh\\\\AppData\\\\Local\\\\Temp\\\\tmpv7o4_dcu', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002CA4E219668>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "dnn = tf.estimator.DNNClassifier(\n",
    "          hidden_units=[512, 128],\n",
    "          feature_columns=[embedding_feature],\n",
    "          n_classes=2,\n",
    "          activation_fn=tf.nn.relu,\n",
    "          dropout=0.1,\n",
    "          optimizer=tf.train.AdagradOptimizer(learning_rate=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Training for step = 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import time\n",
    "\n",
    "TOTAL_STEPS = 1500\n",
    "STEP_SIZE = 100\n",
    "for step in range(0, TOTAL_STEPS+1, STEP_SIZE):\n",
    "    print()\n",
    "    print('-'*100)\n",
    "    print('Training for step =', step)\n",
    "    start_time = time.time()\n",
    "    dnn.train(input_fn=train_input_fn, steps=STEP_SIZE)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print('Train Time (s):', elapsed_time)\n",
    "    print('Eval Metrics (Train):', dnn.evaluate(input_fn=predict_train_input_fn))\n",
    "    print('Eval Metrics (Validation):', dnn.evaluate(input_fn=predict_val_input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
